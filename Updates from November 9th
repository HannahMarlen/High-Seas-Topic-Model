# Topic Model about the Future of the High Seas

# Load all the packages

library(quanteda)
library(tidyverse)
library(tidytext)
library(tm)
library(topicmodels)
library(readxl)
library(ggplot2)
library(servr)

# Loading the Data 

library(readxl)
Kopie_von_scopus_v2 <- read_excel("~/MA_Data/Kopie von scopus_v2.xls")
View(Kopie_von_scopus_v2)

# Tidying the Data

unnest(Kopie_von_scopus_v2, cols = c(1,2))

too_words <- tibble(
  word = c("paper", "study","aim", "sea", "ocean", "marine", "water")
)

tidy_abstracts <- Kopie_von_scopus_v2 %>%
  unnest_tokens(word, 2) %>%
  anti_join(stop_words) %>%
  anti_join(too_words) %>%
  filter(word != "abstract")

tidy_abstracts %>%
  count(word, sort = TRUE)


# Calculating the Frequency of Terms (tdf_idf)

x <- tidy_abstracts %>%
  count(Title, word, sort = TRUE) %>%
  bind_tf_idf(word, Title, n) %>%
  arrange(-tf_idf) %>%
  group_by(Title) %>%
  top_n(3)


# Creating the Document Term Matrix

dtm <- tidy_abstracts %>% 
  select(Abstract, Title) %>%
  unique() %>% 
  unnest_tokens(word, Abstract) %>% 
  anti_join(stop_words) %>% 
  anti_join(too_words) %>%
  filter(!str_detect(word, "[:digit:]")) %>%
  mutate(word = textstem::lemmatize_words(word)) %>% 
  group_by(Title) %>%
  count(word, sort = TRUE) %>%
  cast_dtm(document = Title, term = word, value = n)

save(dtm, file = "High-Seas-Topic-Model")

# Calculating the best fitting algorithm 

SEED <- 2010
k <- 15

tset.TM <- list (
  VEM0 = LDA(dtm, k=k, control = list ( seed = SEED)),
  VEM_fixed= LDA(dtm, k=k, control= list (estimate.alpha = F, seed = SEED)),
  Gibbs = LDA (dtm, k=k, method ="Gibbs", control = list (seed = SEED, burnin= 1000, thin = 100, iter= 1000)),
  CTM = CTM (dtm, k=k, control = list(seed = SEED, var= list (tol= 10^-4), em= list (tol = 10^-3))))

sapply (tset.TM[1:3], slot, "alpha")

# Now look at some stats to see which algorithm is the best!


df_stats <- tibble(
model = names(lapply(tset.TM, logLik)),
loglik = as.numeric(lapply(tset.TM, logLik)), # maximize loglik
entropy = lapply (tset.TM, function (x) 
mean(apply(posterior(x)$topics,
1, function (z) - sum(z * log(z))))) %>% as.numeric() # maximize entropy
)


perp <- lapply(tset.TM[c(1,2,4)], perplexity) # minimize perplexity

perp$Gibbs <- NA # You can't calculate perplexity for Gibbs



# Finding the ideal number of topics

k <- c(5,10,25,50,100)

topicNumber.TM <- map(
  .x = k,
  .f = function(x) {
    LDA(dtm, k = x, control= list (seed = SEED), method = "Gibbs")  # Obviously put the best fitting algorithm here
  })
save(tset.TM1, topicNumber.TM, file = "High-Seas_Topic-Model")


# This shows your topics and their x keywords

Terms <- terms(tset.TM1[["Gibbs"]], 10)

Terms1 <- terms(tset.TM[["Gibbs"]], 8)

Terms2 <- terms(tset.TM1[["Gibbs"]], 30)



#To save and load all objects in the environment

save.image(file='myEnvironment.RData')
load('myEnvironment.RData')


#Intertopic Distance Map Creation // absolute chaos version!

library(LDAvis)
library(servr)

#create a matrix from the dtm

m <- as.matrix(dtm) 
z <- rowSums(m>0) 


#Set the values of a few summary statistics of the corpus and model

# You can see what your topic model contains, you can look at its slots!

slotNames(topicNumber.TM[[2]])
slot(topicNumber.TM[[2]], "gamma") 

theta <- slot(topicNumber.TM[[2]], "gamma")
phi <- slot(topicNumber.TM[[2]], "beta")
doc.length <- z
vocab <- slot(topicNumber.TM[[2]], "terms")

dt <- dim(theta)    
dp <- dim(phi)

N <- sum(doc.length)  # number of tokens in the data
W <- length(vocab)  # number of terms in the vocab
D <- length(doc.length)  # number of documents in the data
K <- dt[2]  # number of topics in the model



# Simple and sleek way to create JSON (doesn't work though)

createJSON(phi = (beta), theta = (gamma), doc.length = (z),
           vocab = (terms), term.frequency = (tf), R = 30,
           lambda.step = 0.01, mds.method = jsPCA, cluster, plot.opts = list(xlab =
                                                                               "PC1", ylab = "PC2"))

#This tests for a frequent error message (but doesn't resolve it)

phi.test <- all.equal(rowSums(phi), rep(1, K), check.attributes = FALSE)
theta.test <- all.equal(rowSums(theta), rep(1, dt[1]), 
                        check.attributes = FALSE)

if (!isTRUE(phi.test)) warning("Rows of phi don't all sum to 1.")
if (!isTRUE(theta.test)) warning("Rows of theta don't all sum to 1.")
doc_length <- vector()
for (i in 1:length(corpus)) {
  +     temp <- paste(corpus[[i]]$content, collapse = ' ')
  +     doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
  + }


# I have no idea if I need this values for anything

topic.frequency <- colSums(theta * doc.length)
topic.proportion <- topic.frequency/sum(topic.frequency)


# Now this is the function that creates the JSON object and worked once for me
topicmodels2LDAvis <- function(x, ...){
       post <- topicmodels::posterior(x)
       if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
       mat <- x@wordassignments
       LDAvis::createJSON(
             phi = post[["terms"]], 
             theta = post[["topics"]],
             vocab = colnames(post[["terms"]]),
             doc.length = slam::row_sums(mat, na.rm = TRUE),
             term.frequency = slam::col_sums(mat, na.rm = TRUE)
         )
   }

# This is the final line that actually creates the visualization, but its very wrong!

result55 <- LDA(dtm, 15)
 serVis(topicmodels2LDAvis(result55))
